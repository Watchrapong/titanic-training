# 1. Import ‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import f1_score, make_scorer
from xgboost import XGBClassifier

# 2. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
data = pd.concat([train, test], sort=False)  # ‡∏£‡∏ß‡∏° train ‡πÅ‡∏•‡∏∞ test ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ Feature Engineering ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô

# 3. Feature Engineering

# 3.1 ‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ä‡∏∑‡πà‡∏≠ (Title) ‡∏à‡∏≤‡∏Å Name
data['Title'] = data['Name'].str.extract(r' ([A-Za-z]+)\.', expand=False)
data['Title'] = data['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 
                                       'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')
data['Title'] = data['Title'].replace({'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs'})

# 3.2 ‡πÅ‡∏õ‡∏•‡∏á Sex ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
data['Sex'] = data['Sex'].map({'male': 0, 'female': 1}).astype(int)

# 3.3 ‡πÄ‡∏ï‡∏¥‡∏° Missing Values
data['Embarked'] = data['Embarked'].fillna(data['Embarked'].mode()[0])
data['Fare'] = data['Fare'].fillna(data['Fare'].median())
data['Age'] = data['Age'].fillna(data['Age'].median())

# 3.4 FareBin ‡πÅ‡∏•‡∏∞ AgeBin (‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°)
data['FareBin'] = pd.qcut(data['Fare'], 4, labels=False)
data['AgeBin'] = pd.cut(data['Age'].astype(int), 5, labels=False)

# 3.5 FamilySize ‡πÅ‡∏•‡∏∞ IsAlone
data['FamilySize'] = data['SibSp'] + data['Parch'] + 1
data['IsAlone'] = (data['FamilySize'] == 1).astype(int)


# 3.6 One-hot encoding ‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö Embarked ‡πÅ‡∏•‡∏∞ Title
data = pd.get_dummies(data, columns=['Embarked', 'Title'], drop_first=True)

data.to_csv('prepare_data.csv', index=False)
print("üìÑ prepare_data.csv created.")


# 4. ‡πÅ‡∏¢‡∏Å‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô train/test ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
train = data[:len(train)]
test = data[len(train):]

# 5. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
feature_cols = ['Sex', 'FareBin', 'AgeBin', 'FamilySize', 'IsAlone'] + \
               [col for col in train.columns if col.startswith('Embarked_') or col.startswith('Title_')]

X = train[feature_cols]
y = train['Survived']
X_test = test[feature_cols]

# 6. ‡πÅ‡∏ö‡πà‡∏á Training/Validation set
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# 7. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• + ‡πÉ‡∏ä‡πâ GridSearchCV ‡∏´‡∏≤ hyperparameter ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)


param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 4],
    'learning_rate': [0.05, 0.1]
}

grid_search = GridSearchCV(estimator=xgb,
                           param_grid=param_grid,
                           scoring=make_scorer(f1_score),
                           cv=5,
                           verbose=1,
                           n_jobs=-1)

grid_search.fit(X_train, y_train)

# 8. ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_val)
f1 = f1_score(y_val, y_pred)
print("Best Parameters:", grid_search.best_params_)
print(f"Validation F1 Score: {f1:.4f}")

# 9. ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ö‡∏ô‡∏ä‡∏∏‡∏î test
pred_test = best_model.predict(X_test)

# 10. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå submission.csv ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Kaggle
submission = pd.DataFrame({
    'PassengerId': test['PassengerId'],
    'Survived': pred_test
})
submission.to_csv('xgboost_submission.csv', index=False)
print("xgboost_submission.csv created.")

from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics ‡∏ï‡πà‡∏≤‡∏á ‡πÜ
accuracy = accuracy_score(y_val, y_pred)
precision = precision_score(y_val, y_pred)
recall = recall_score(y_val, y_pred)
cm = confusion_matrix(y_val, y_pred)

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
print("Evaluation Metrics on Validation Set:")
print(f"Accuracy  : {accuracy:.4f}")
print(f"Precision : {precision:.4f}")
print(f"Recall    : {recall:.4f}")
print(f"F1 Score  : {f1:.4f}")

# ‡πÅ‡∏™‡∏î‡∏á Confusion Matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Not Survived", "Survived"])
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

# 12. ‡πÅ‡∏™‡∏î‡∏á Feature Importance
from xgboost import plot_importance, plot_tree

plot_importance(best_model, importance_type='gain', max_num_features=10, height=0.5)
plt.title("Top 10 Important Features (by Gain)")
plt.show()

from xgboost import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(20, 10))
plot_tree(best_model, tree_idx=0, rankdir='LR')
plt.title("XGBoost Tree Visualization (Tree 0)", fontsize=16)
plt.show()
